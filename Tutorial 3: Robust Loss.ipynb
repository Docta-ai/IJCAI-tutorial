{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Learning algorithms that handle noisy labels\n",
    "\n",
    "This module will provide examples showing how one would implement a learning algorithm specifically designed for handling noisy labels. \n",
    "\n",
    "We will introduce robust loss functions that are ready to be plugged into the empirical risk minimization (ERM) framework. The specific loss functions include [forward/backward loss correction](https://openaccess.thecvf.com/content_cvpr_2017/html/Patrini_Making_Deep_Neural_CVPR_2017_paper.html), loss reweighting [[paper 1](https://proceedings.neurips.cc/paper/2019/hash/9308b0d6e5898366a4a986bc33f3d3e7-Abstract.html), [paper 2](https://ieeexplore.ieee.org/abstract/document/7159100?casa_token=_bObUprbmdIAAAAA:_nx8tU9bE6_6MUw2-6A72c2K_pUtOcEyAm40V70rsaRFRTnPildNJ7qo6FT5mvdOJgP4nwTb8g)], peer loss [[paper 1](http://proceedings.mlr.press/v119/liu20e.html), [paper 2](https://openreview.net/forum?id=WesiCoRVQ15)], [bi-tempered loss](https://proceedings.neurips.cc/paper/2019/hash/8cd7775f9129da8b5bf787a063d8426e-Abstract.html), and label smoothing [[paper 1](http://proceedings.mlr.press/v119/lukasik20a.html?ref=https://githubhelp.com), [paper 2](https://proceedings.mlr.press/v162/wei22b.html)]. We will also discuss other approaches, including [Co-Teaching](https://proceedings.neurips.cc/paper/8072-co-teaching-robust-training-o), and [DivideMix](https://openreview.net/forum?id=HJgExaVtwr), which employ two neural networks to supervise each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The role of noise transition matrix\n",
    "\n",
    "For the K-class classification task, we have\n",
    "\n",
    "$\n",
    "    \\underbrace{\\begin{bmatrix}\n",
    "    \\mathbb{P}(\\widetilde{Y}=1)\\\\\n",
    "    \\mathbb{P}(\\widetilde{Y}=2)\\\\\n",
    "    ...\\\\\n",
    "    \\mathbb{P}(\\widetilde{Y}=K)\n",
    "    \\end{bmatrix}}_{\\text{Observation}}=\\underbrace{\\begin{bmatrix}\n",
    "    \\mathbb{P}(\\widetilde{Y}=1|Y=1) & \\mathbb{P}(\\widetilde{Y}=1|Y=2) & ... & \\mathbb{P}(\\widetilde{Y}=1|Y=K)\\\\\n",
    "    \\mathbb{P}(\\widetilde{Y}=2|Y=1) & \\mathbb{P}(\\widetilde{Y}=2|Y=2) & ... & \\mathbb{P}(\\widetilde{Y}=2|Y=K)\\\\\n",
    "    ...& ... & ... & ...\\\\ \\mathbb{P}(\\widetilde{Y}=K|Y=1) & \\mathbb{P}(\\widetilde{Y}=1|Y=2) & ... & \\mathbb{P}(\\widetilde{Y}=1|Y=K)\n",
    "    \\end{bmatrix}}_{\\text{Class-dependent noise transition matrix }T^{\\intercal}}\\cdot \\underbrace{\\begin{bmatrix}\n",
    "    \\mathbb{P}(Y=1)\\\\\n",
    "    \\mathbb{P}(Y=2)\\\\\n",
    "    ...\\\\\n",
    "   \\mathbb{P}(Y=K)\n",
    "    \\end{bmatrix}}_{\\text{Hidden truth}}.\n",
    "$\n",
    "\n",
    "A simplified binary case can be defined as (Class $+1$, Class $-1$):\n",
    "\n",
    "$\n",
    "     \\begin{bmatrix}\n",
    "    \\mathbb{P}(\\widetilde{Y}=+1)\\\\\n",
    "    \\mathbb{P}(\\widetilde{Y}=-1)\n",
    "    \\end{bmatrix}  =\\underbrace{\\begin{bmatrix}\n",
    "    1-e_+ & e_-  \\\\\n",
    "    e_+ & 1-e_-  \n",
    "    \\end{bmatrix}}_{T_{2\\times 2}^{\\intercal}}\\cdot \\underbrace{\\begin{bmatrix}\n",
    "    \\mathbb{P}(Y=+1)\\\\\n",
    "    \\mathbb{P}(Y=-1)\n",
    "    \\end{bmatrix}}_{\\text{Hidden truth}},\n",
    "$\n",
    "\n",
    "where $e_+:=\\mathbb{P}(\\widetilde{Y}=-1|Y=+1), e_-:=\\mathbb{P}(\\widetilde{Y}=+1|Y=-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Loss Correction\n",
    " \n",
    "**Loss correction**\n",
    "\n",
    "For the noisy data sample $(x, \\tilde{y})$, suppose the clean label of $x$ is $y$:\n",
    "$$\\ell_{\\text{LC}}(h(x), \\tilde{y}):=\\frac{(1-e_{-\\tilde{y}})}{1-e_+-e_-}\\cdot {\\color{red}{\\ell(h(x),\\tilde{y})}}-\\frac{e_{\\tilde{y}}}{1-e_+-e_-}\\cdot {\\color{blue}{\\ell(h(x),-\\tilde{y})}}.$$\n",
    "\n",
    "**<font color='red'> Remark: </font>**\n",
    "$-\\tilde{y}$ simply encodes the class differs from $\\tilde{y}$.\n",
    "\n",
    "**Motivation of loss correction** (i.e., for class $y=+1$)\n",
    "\n",
    "$$\\mathbb{E}_{\\tilde{y}|y=+1}[\\ell_{\\text{LC}}(h(x), \\tilde{y})]=\\underbrace{(1-e_+)}_{\\mathbb{P}(\\tilde{y}=+1|y=+1)}\\cdot \\ell_{\\text{LC}}(h(x), \\tilde{y}=+1)+\\underbrace{e_+}_{\\mathbb{P}(\\tilde{y}=-1|y=+1)}\\cdot \\ell_{\\text{LC}}(h(x), \\tilde{y}=-1).$$\n",
    "\n",
    "Substitute $\\ell_{\\text{LC}}(h(x), \\tilde{y}=+1)$ and $\\ell_{\\text{LC}}(h(x), \\tilde{y}=-1)$, we have\n",
    "\n",
    "$$\\mathbb{E}_{\\tilde{y}|y=+1}[\\ell_{\\text{LC}}(h(x), \\tilde{y})]=\\underbrace{\\ell(h(x), y=+1)}_{{\\color{red}{\\text{Loss on the clean data!}}}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
